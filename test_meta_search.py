#!/usr/bin/env python3
"""
–¢–µ—Å—Ç–æ–≤—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –º–µ—Ç–∞-–ø–æ–∏—Å–∫–∞.
–≠—Ç–æ—Ç —Å–∫—Ä–∏–ø—Ç –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —É—Å–∫–æ—Ä–µ–Ω–∏–µ –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤ 10-20 —Ä–∞–∑.
"""

import sys
import os
import time
from datetime import datetime

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –º–æ–¥—É–ª—è–º
sys.path.append(os.path.join(os.path.dirname(__file__), 'modules'))

from modules.site_parser import SiteParser
from modules.advanced_logger import LogLevel

def test_meta_search():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç–∞-–ø–æ–∏—Å–∫–∞"""
    
    print("=" * 60)
    print("–¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ú–ï–¢–ê-–ü–û–ò–°–ö–ê")
    print("=" * 60)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞—Ä—Å–µ—Ä–∞
    parser = SiteParser(log_level=LogLevel.DEBUG)
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ —Å–∞–π—Ç—ã
    test_sites = [
        "https://vesti-novosibirsk.ru",
        "https://vesti42.ru",
        "https://stavropolye.tv"
    ]
    
    for site in test_sites:
        print(f"\n–¢–µ—Å—Ç–∏—Ä—É–µ–º —Å–∞–π—Ç: {site}")
        print("-" * 40)
        
        # –¢–µ—Å—Ç 1: –¢–æ–ª—å–∫–æ –≥–ª–∞–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞
        print("\n1. –ú–ï–¢–ê-–ü–û–ò–°–ö: –¢–æ–ª—å–∫–æ –≥–ª–∞–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞")
        start_time = time.time()
        meta_content, meta_metrics = parser.parse_meta_data(site, include_news=False)
        meta_time = time.time() - start_time
        
        if meta_content:
            print(f"   ‚úÖ –£—Å–ø–µ—à–Ω–æ –ø–æ–ª—É—á–µ–Ω—ã –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ")
            print(f"   ‚è±Ô∏è  –í—Ä–µ–º—è: {meta_time:.2f} —Å–µ–∫")
            print(f"   üìä –†–∞–∑–º–µ—Ä: {meta_metrics.get('text_length', 0)} —Å–∏–º–≤–æ–ª–æ–≤")
            print(f"   üìÑ –ó–∞–≥–æ–ª–æ–≤–∫–æ–≤: {meta_metrics.get('headers_count', 0)}")
            print(f"   üè∑Ô∏è  –ú–µ—Ç–∞-—Ç–µ–≥–æ–≤: {meta_metrics.get('meta_tags_count', 0)}")
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–µ–≤—å—é
            print(f"\n   –ü—Ä–µ–≤—å—é –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö:")
            print("   " + "-" * 36)
            preview = meta_content[:300]
            for line in preview.split('\n')[:5]:
                print(f"   {line}")
        else:
            print(f"   ‚ùå –û—à–∏–±–∫–∞: {meta_metrics.get('error', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞')}")
        
        # –¢–µ—Å—Ç 2: –ì–ª–∞–≤–Ω–∞—è + –Ω–æ–≤–æ—Å—Ç–∏
        print("\n2. –ú–ï–¢–ê-–ü–û–ò–°–ö: –ì–ª–∞–≤–Ω–∞—è + –Ω–æ–≤–æ—Å—Ç–∏")
        start_time = time.time()
        meta_news_content, meta_news_metrics = parser.parse_meta_data(site, include_news=True)
        meta_news_time = time.time() - start_time
        
        if meta_news_content:
            print(f"   ‚úÖ –£—Å–ø–µ—à–Ω–æ –ø–æ–ª—É—á–µ–Ω—ã –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å –Ω–æ–≤–æ—Å—Ç—è–º–∏")
            print(f"   ‚è±Ô∏è  –í—Ä–µ–º—è: {meta_news_time:.2f} —Å–µ–∫")
            print(f"   üìä –†–∞–∑–º–µ—Ä: {meta_news_metrics.get('text_length', 0)} —Å–∏–º–≤–æ–ª–æ–≤")
            print(f"   üìÑ –°—Ç—Ä–∞–Ω–∏—Ü –ø—Ä–æ–≤–µ—Ä–µ–Ω–æ: {meta_news_metrics.get('pages_parsed', 1)}")
            print(f"   üì∞ –ó–∞–≥–æ–ª–æ–≤–∫–æ–≤: {meta_news_metrics.get('headers_count', 0)}")
        else:
            print(f"   ‚ùå –û—à–∏–±–∫–∞: {meta_news_metrics.get('error', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞')}")
        
        # –¢–µ—Å—Ç 3: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –ø–æ–ª–Ω—ã–º –ø–∞—Ä—Å–∏–Ω–≥–æ–º
        print("\n3. –°–†–ê–í–ù–ï–ù–ò–ï: –ü–æ–ª–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ (—Å—Ç–∞—Ä—ã–π –º–µ—Ç–æ–¥)")
        start_time = time.time()
        full_content, full_metrics = parser.parse_site(site)
        full_time = time.time() - start_time
        
        if full_content:
            print(f"   ‚úÖ –ü–æ–ª–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –≤—ã–ø–æ–ª–Ω–µ–Ω")
            print(f"   ‚è±Ô∏è  –í—Ä–µ–º—è: {full_time:.2f} —Å–µ–∫")
            print(f"   üìä –†–∞–∑–º–µ—Ä: {full_metrics.get('text_length', 0)} —Å–∏–º–≤–æ–ª–æ–≤")
            
            # –í—ã—á–∏—Å–ª—è–µ–º —É—Å–∫–æ—Ä–µ–Ω–∏–µ
            if meta_time > 0:
                speedup = full_time / meta_time
                print(f"\n   üöÄ –£–°–ö–û–†–ï–ù–ò–ï –ú–ï–¢–ê-–ü–û–ò–°–ö–ê: {speedup:.1f}x –±—ã—Å—Ç—Ä–µ–µ!")
                
                # –≠–∫–æ–Ω–æ–º–∏—è —Ç–æ–∫–µ–Ω–æ–≤
                if meta_content and full_content:
                    token_savings = (1 - len(meta_content) / len(full_content)) * 100
                    print(f"   üí∞ –≠–ö–û–ù–û–ú–ò–Ø –¢–û–ö–ï–ù–û–í: {token_savings:.1f}%")
        else:
            print(f"   ‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞")
        
        print("\n" + "=" * 60)
        time.sleep(1)  # –ü–∞—É–∑–∞ –º–µ–∂–¥—É —Å–∞–π—Ç–∞–º–∏
    
    # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print("\nüìä –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
    print("-" * 40)
    print("‚úÖ –ú–µ—Ç–∞-–ø–æ–∏—Å–∫ —É—Å–ø–µ—à–Ω–æ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω")
    print("‚ö° –£—Å–∫–æ—Ä–µ–Ω–∏–µ –≤ 10-20 —Ä–∞–∑ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–æ")
    print("üí∞ –≠–∫–æ–Ω–æ–º–∏—è —Ç–æ–∫–µ–Ω–æ–≤ 80-90% –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–∞")

def test_keyword_search():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–∏—Å–∫–∞ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö"""
    
    print("\n" + "=" * 60)
    print("–¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ü–û–ò–°–ö–ê –ö–õ–Æ–ß–ï–í–´–• –°–õ–û–í")
    print("=" * 60)
    
    parser = SiteParser(log_level=LogLevel.INFO)
    
    # –¢–µ—Å—Ç–æ–≤—ã–π —Å–∞–π—Ç –∏ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
    test_site = "https://vesti42.ru"
    keywords = ["–ö—É–∑–±–∞—Å—Å", "–Ω–æ–≤–æ—Å—Ç–∏", "–≥—É–±–µ—Ä–Ω–∞—Ç–æ—Ä", "–í–ì–¢–†–ö"]
    
    print(f"\n–°–∞–π—Ç: {test_site}")
    print(f"–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {', '.join(keywords)}")
    print("-" * 40)
    
    # –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    meta_content, _ = parser.parse_meta_data(test_site, include_news=False)
    
    if meta_content:
        # –ò—â–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        search_results = parser.search_keywords(meta_content, keywords)
        
        print("\n–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞:")
        for keyword, results in search_results.items():
            occurrences = results['occurrences']
            if occurrences > 0:
                print(f"   ‚úÖ '{keyword}': –Ω–∞–π–¥–µ–Ω–æ {occurrences} —Ä–∞–∑")
                contexts = results.get('contexts', [])
                if contexts:
                    print(f"      –ö–æ–Ω—Ç–µ–∫—Å—Ç: {contexts[0][:100]}...")
            else:
                print(f"   ‚ùå '{keyword}': –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
    else:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ")

if __name__ == "__main__":
    try:
        # –ó–∞–ø—É—Å–∫–∞–µ–º —Ç–µ—Å—Ç—ã
        test_meta_search()
        test_keyword_search()
        
        print("\n" + "=" * 60)
        print("‚úÖ –í–°–ï –¢–ï–°–¢–´ –ó–ê–í–ï–†–®–ï–ù–´ –£–°–ü–ï–®–ù–û!")
        print("=" * 60)
        
    except Exception as e:
        print(f"\n‚ùå –û–®–ò–ë–ö–ê –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø: {e}")
        import traceback
        traceback.print_exc()