"""
–¢–µ—Å—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –æ—à–∏–±–∫–∏ —Å –Ω–µ–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π parsed_content
"""

import sys
import os
from datetime import datetime, timedelta

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –º–æ–¥—É–ª—è–º
sys.path.append(os.path.join(os.path.dirname(__file__), 'modules'))

from modules.database import VGTRKDatabase
from modules.site_parser import SiteParser
from modules.gigachat_client import GigaChatClient
from modules.advanced_logger import AdvancedLogger, LogLevel, get_logger
from modules.scrapy_parser import ScrapyParser
from config.settings import GIGACHAT_API_KEY, GIGACHAT_CLIENT_ID

def test_sitemap_search():
    """–¢–µ—Å—Ç —Ä–µ–∂–∏–º–∞ sitemap_search —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö"""
    
    print("=" * 60)
    print("–¢–ï–°–¢: –†–µ–∂–∏–º sitemap_search —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º parsed_content")
    print("=" * 60)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
    logger = get_logger(LogLevel.DEBUG)
    scrapy_parser = ScrapyParser(logger=logger)
    
    # –¢–µ—Å—Ç–æ–≤—ã–π —Ñ–∏–ª–∏–∞–ª
    test_filial = {
        'id': 14,
        'name': '–ì–¢–†–ö –°–∞—Ö–∞',
        'website_url': 'https://gtrksakha.ru'
    }
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
    test_queries = [
        {'id': 1, 'query_text': '–∑–¥—Ä–∞–≤–æ–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ'},
        {'id': 2, 'query_text': '–∫–æ–Ω–≥—Ä–µ—Å—Å'}
    ]
    
    print(f"\nüîç –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ñ–∏–ª–∏–∞–ª: {test_filial['name']}")
    print(f"   –°–∞–π—Ç: {test_filial['website_url']}")
    print(f"   –ó–∞–ø—Ä–æ—Å—ã: {[q['query_text'] for q in test_queries]}")
    
    try:
        # –ò–º–∏—Ç–∏—Ä—É–µ–º –±–ª–æ–∫ –∏–∑ process_monitoring
        website = test_filial['website_url']
        filial_name = test_filial['name']
        keywords = [q['query_text'] for q in test_queries]
        search_days = 7
        
        print(f"\nüï∑Ô∏è –ó–∞–ø—É—Å–∫–∞–µ–º Sitemap –ø–æ–∏—Å–∫ –∑–∞ {search_days} –¥–Ω–µ–π...")
        
        # –ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ Sitemap
        sitemap_results = scrapy_parser.search_with_sitemap(
            website,
            keywords,
            days=search_days,
            max_articles=50
        )
        
        # –í–ê–ñ–ù–û: –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
        parsed_content = None  # Sitemap —Ä–µ–∂–∏–º –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç parsed_content
        parse_metrics = {'mode': 'sitemap', 'articles_checked': len(sitemap_results) if sitemap_results else 0}
        
        print(f"\n‚úÖ –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã:")
        print(f"   parsed_content: {type(parsed_content)}")
        print(f"   parse_metrics: {parse_metrics}")
        
        if sitemap_results:
            print(f"\nüìä –ù–∞–π–¥–µ–Ω–æ {len(sitemap_results)} —Å—Ç–∞—Ç–µ–π")
            for idx, article in enumerate(sitemap_results[:3], 1):
                print(f"\n   {idx}. {article['title'][:100]}")
                print(f"      URL: {article['url']}")
                if article.get('date'):
                    print(f"      –î–∞—Ç–∞: {article['date']}")
                if article.get('keywords'):
                    print(f"      –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {', '.join(article['keywords'][:3])}")
        else:
            print("\n‚ö†Ô∏è Sitemap –Ω–µ –Ω–∞–π–¥–µ–Ω –∏–ª–∏ –ø—É—Å—Ç–æ–π")
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –æ—à–∏–±–∫–∏
            parse_metrics = {'error': 'Sitemap –Ω–µ –Ω–∞–π–¥–µ–Ω –∏–ª–∏ –ø—É—Å—Ç–æ–π'}
            parsed_content = None
            
            print(f"\n‚úÖ –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã –ø—Ä–∏ –æ—à–∏–±–∫–µ:")
            print(f"   parsed_content: {type(parsed_content)}")
            print(f"   parse_metrics: {parse_metrics}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–∞–ª—å—à–µ
        print(f"\nüîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö:")
        
        # –≠—Ç–∞ –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–µ –¥–æ–ª–∂–Ω–∞ –≤—ã–∑–≤–∞—Ç—å –æ—à–∏–±–∫—É
        if parsed_content is not None:
            print(f"   parsed_content —Å–æ–¥–µ—Ä–∂–∏—Ç –¥–∞–Ω–Ω—ã–µ: {len(parsed_content)} —Å–∏–º–≤–æ–ª–æ–≤")
        else:
            print(f"   parsed_content = None (–æ–∂–∏–¥–∞–µ–º–æ –¥–ª—è sitemap —Ä–µ–∂–∏–º–∞)")
        
        print(f"   parse_metrics —Å–æ–¥–µ—Ä–∂–∏—Ç: {parse_metrics}")
        
        print("\n‚úÖ –¢–ï–°–¢ –ü–†–û–ô–î–ï–ù: –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∞–≤–∏–ª—å–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã!")
        
    except NameError as e:
        print(f"\n‚ùå –û–®–ò–ë–ö–ê: –ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞: {e}")
        return False
    except Exception as e:
        print(f"\n‚ùå –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    return True

def test_rss_search():
    """–¢–µ—Å—Ç —Ä–µ–∂–∏–º–∞ rss_search –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö"""
    
    print("\n" + "=" * 60)
    print("–¢–ï–°–¢: –†–µ–∂–∏–º rss_search")
    print("=" * 60)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
    logger = get_logger(LogLevel.INFO)
    site_parser = SiteParser(log_level=LogLevel.INFO)
    
    # –¢–µ—Å—Ç–æ–≤—ã–π —Ñ–∏–ª–∏–∞–ª
    test_filial = {
        'id': 14,
        'name': '–ì–¢–†–ö –°–∞—Ö–∞',
        'website_url': 'https://gtrksakha.ru'
    }
    
    print(f"\nüì° –¢–µ—Å—Ç–∏—Ä—É–µ–º RSS –¥–ª—è: {test_filial['name']}")
    
    try:
        website = test_filial['website_url']
        
        # RSS –ø–∞—Ä—Å–∏–Ω–≥
        rss_data, parse_metrics = site_parser.parse_rss_feed(website)
        
        if rss_data:
            # –§–æ—Ä–º–∏—Ä—É–µ–º parsed_content –∏–∑ RSS
            parsed_content = f"=== RSS –õ–ï–ù–¢–ê {test_filial['name']} ===\n"
            parsed_content += f"–ö–∞–Ω–∞–ª: {rss_data.get('channel_title', '')}\n"
            parsed_content += f"–û–ø–∏—Å–∞–Ω–∏–µ: {rss_data.get('channel_description', '')}\n\n"
            
            items_count = 0
            for item in rss_data.get('items', []):
                parsed_content += f"--- –ù–û–í–û–°–¢–¨ ---\n"
                parsed_content += f"–ó–∞–≥–æ–ª–æ–≤–æ–∫: {item['title']}\n"
                parsed_content += f"–û–ø–∏—Å–∞–Ω–∏–µ: {item['description']}\n"
                parsed_content += f"–°—Å—ã–ª–∫–∞: {item['link']}\n"
                parsed_content += f"–î–∞—Ç–∞: {item['pubDate']}\n\n"
                items_count += 1
                if items_count >= 3:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 3 –¥–ª—è —Ç–µ—Å—Ç–∞
                    break
            
            print(f"\n‚úÖ RSS –ø–æ–ª—É—á–µ–Ω: {len(rss_data.get('items', []))} –Ω–æ–≤–æ—Å—Ç–µ–π")
            print(f"   parsed_content: {len(parsed_content)} —Å–∏–º–≤–æ–ª–æ–≤")
            print(f"   parse_metrics: {parse_metrics}")
        else:
            parsed_content = None
            print("\n‚ö†Ô∏è RSS –Ω–µ –Ω–∞–π–¥–µ–Ω")
            print(f"   parsed_content: {type(parsed_content)}")
            print(f"   parse_metrics: {parse_metrics}")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
        if parsed_content:
            print(f"\n‚úÖ –¢–ï–°–¢ RSS –ü–†–û–ô–î–ï–ù: parsed_content —Å–æ–¥–µ—Ä–∂–∏—Ç {len(parsed_content)} —Å–∏–º–≤–æ–ª–æ–≤")
        else:
            print(f"\n‚úÖ –¢–ï–°–¢ RSS –ü–†–û–ô–î–ï–ù: parsed_content = None (RSS –Ω–µ –Ω–∞–π–¥–µ–Ω)")
        
    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞ –≤ RSS —Ç–µ—Å—Ç–µ: {e}")
        return False
    
    return True

def test_meta_search():
    """–¢–µ—Å—Ç —Ä–µ–∂–∏–º–∞ meta_search –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö"""
    
    print("\n" + "=" * 60)
    print("–¢–ï–°–¢: –†–µ–∂–∏–º meta_search")
    print("=" * 60)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
    logger = get_logger(LogLevel.INFO)
    site_parser = SiteParser(log_level=LogLevel.INFO)
    
    # –¢–µ—Å—Ç–æ–≤—ã–π —Ñ–∏–ª–∏–∞–ª
    test_filial = {
        'id': 14,
        'name': '–ì–¢–†–ö –°–∞—Ö–∞',
        'website_url': 'https://gtrksakha.ru'
    }
    
    print(f"\nüìÑ –¢–µ—Å—Ç–∏—Ä—É–µ–º –º–µ—Ç–∞-–ø–∞—Ä—Å–∏–Ω–≥ –¥–ª—è: {test_filial['name']}")
    
    try:
        website = test_filial['website_url']
        
        # –ú–µ—Ç–∞-–ø–∞—Ä—Å–∏–Ω–≥
        parsed_content, parse_metrics = site_parser.parse_meta_data(website, include_news=False)
        
        if parsed_content:
            print(f"\n‚úÖ –ú–µ—Ç–∞-–¥–∞–Ω–Ω—ã–µ –ø–æ–ª—É—á–µ–Ω—ã:")
            print(f"   parsed_content: {len(parsed_content)} —Å–∏–º–≤–æ–ª–æ–≤")
            print(f"   –°—Ç—Ä–∞–Ω–∏—Ü: {parse_metrics.get('pages_parsed', 0)}")
            print(f"   –ó–∞–≥–æ–ª–æ–≤–∫–æ–≤: {parse_metrics.get('headers_count', 0)}")
            print(f"   –ú–µ—Ç–∞-—Ç–µ–≥–æ–≤: {parse_metrics.get('meta_tags_count', 0)}")
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 200 —Å–∏–º–≤–æ–ª–æ–≤
            preview = parsed_content[:200] + "..." if len(parsed_content) > 200 else parsed_content
            print(f"\n   –ü—Ä–µ–≤—å—é –∫–æ–Ω—Ç–µ–Ω—Ç–∞:\n{preview}")
        else:
            print("\n‚ö†Ô∏è –ú–µ—Ç–∞-–¥–∞–Ω–Ω—ã–µ –Ω–µ –ø–æ–ª—É—á–µ–Ω—ã")
            print(f"   parsed_content: {type(parsed_content)}")
            print(f"   parse_metrics: {parse_metrics}")
        
        print(f"\n‚úÖ –¢–ï–°–¢ META –ü–†–û–ô–î–ï–ù!")
        
    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞ –≤ meta —Ç–µ—Å—Ç–µ: {e}")
        return False
    
    return True

if __name__ == "__main__":
    print("üöÄ –ó–ê–ü–£–°–ö –¢–ï–°–¢–û–í –ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø parsed_content\n")
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –≤—Å–µ —Ç–µ—Å—Ç—ã
    tests = [
        ("Sitemap Search", test_sitemap_search),
        ("RSS Search", test_rss_search),
        ("Meta Search", test_meta_search)
    ]
    
    results = []
    for test_name, test_func in tests:
        try:
            success = test_func()
            results.append((test_name, success))
        except Exception as e:
            print(f"\n‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ —Ç–µ—Å—Ç–µ {test_name}: {e}")
            results.append((test_name, False))
    
    # –ò—Ç–æ–≥–∏
    print("\n" + "=" * 60)
    print("–ò–¢–û–ì–ò –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø")
    print("=" * 60)
    
    for test_name, success in results:
        status = "‚úÖ –ü–†–û–ô–î–ï–ù" if success else "‚ùå –ü–†–û–í–ê–õ–ï–ù"
        print(f"{test_name}: {status}")
    
    all_passed = all(success for _, success in results)
    if all_passed:
        print("\nüéâ –í–°–ï –¢–ï–°–¢–´ –ü–†–û–ô–î–ï–ù–´ –£–°–ü–ï–®–ù–û!")
    else:
        print("\n‚ö†Ô∏è –ù–µ–∫–æ—Ç–æ—Ä—ã–µ —Ç–µ—Å—Ç—ã –ø—Ä–æ–≤–∞–ª–µ–Ω—ã, —Ç—Ä–µ–±—É–µ—Ç—Å—è –¥–æ—Ä–∞–±–æ—Ç–∫–∞")